{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая линейная регрессия\n",
    "\n",
    "Пусть $y$ - целевая переменная\n",
    "\n",
    "$$y=\\beta_0+\\beta_1 \\cdot x + \\epsilon$$ \n",
    "\n",
    "Подгоняем модель вида $$y=\\beta_0+\\beta_1 \\cdot x$$ \n",
    "\n",
    "### Допущения\n",
    "\n",
    "1. Вид зависимости $y=\\beta_0+\\beta_1 \\cdot x + \\epsilon$\n",
    "\n",
    "2. Независимость наблюдений друг от друга\n",
    "\n",
    "3. $\\mathbb{D}\\epsilon = \\sigma^2$ - дисперсия ошибок постоянна (гомоскедастичность) \n",
    "\n",
    "4. $\\epsilon \\sim N(0,\\sigma^2)$\n",
    "\n",
    "\n",
    "# Проблемы при использовании\n",
    "\n",
    "1. Выбросы в данных\n",
    "\n",
    "2. Коррелированность в признаках\n",
    "\n",
    "3. Неадекватность выбора линейной модели \n",
    "\n",
    "4. Интерпретация причинно-следственных связей\n",
    "\n",
    "### Метод наименьших квадратов\n",
    "Минимизируюя сумму квадратов ошибок\n",
    "\n",
    "$$RSS(\\beta_0,\\beta_1) = \\sum_{t=1}^n(y-(\\beta_0+\\beta_1 \\cdot x))^2$$\n",
    "\n",
    "найдем оценки параметров $\\beta_0$ и $\\beta_1$. Вычислим частные производный и, приравнивая их нулю, получим\n",
    "\n",
    "$$\\frac{\\partial RSS(\\beta_0,\\beta_1)}{\\partial{\\beta_0}}= 0$$\n",
    "\n",
    "$$\\frac{\\partial RSS(\\beta_0,\\beta_1)}{\\partial{\\beta_1}}= 0$$\n",
    "\n",
    "получим\n",
    "\n",
    "$$\\widehat{\\beta}_1=\\frac{\\sum_{t=1}^n(y-\\overline y)(x-\\overline x)}{\\sum_{t=1}^n(x-\\overline x)^2}$$\n",
    "\n",
    "$$\\widehat{\\beta_0}=\\overline{y} -\\beta_1x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = pd.read_csv('SalaryData.csv')\n",
    "salary = salary.rename(columns={'Experience Years': 'Experience'})\n",
    "y = salary['Salary']\n",
    "x = salary[['Experience']]\n",
    "salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant\n",
    "model = ols(\"Salary ~ Experience \", data=salary)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters: \", result.params)\n",
    "print(\"Standard errors: \", result.bse)\n",
    "print(\"Predicted values: \", result.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Разбор вывода**\n",
    "\n",
    "**Dep. Variable**: Salary - то, что мы пытаемся объяснить\n",
    "\n",
    "**Number of observations** - кол-во наблюдений\n",
    "\n",
    "**Df Residuals** - кол-во наблюдений - (кол-во переменных в модели + 1 (константа))\n",
    "\n",
    "**Df Model** - (кол-во переменных в модели)\n",
    "\n",
    "**Intercept** = $\\beta_0$ из формулы модели\n",
    "\n",
    "**Experience** = $\\beta_1$ из формулы модели (приращение на единицу роста)\n",
    "\n",
    "**Standard Error of Parameters** - среднеквадратичное отклонение оценок параметров\n",
    "\n",
    "**t-statistics** является результатом деления $coef/std~err$. Статистика для проверки нулевой гипотезы о равенстве соответствующего коэффициента нулю.\n",
    "\n",
    "**P>|t|** - p-value для проверки описанной выше гипотезы\n",
    "\n",
    "**[0.025 0.975]** - доверительные интервалы для значений параметров\n",
    "\n",
    "**R-Squared (коэффициент детерминации)** - показывает долю объясненной дисперсии целевой переменной с помощью предикторов. Изменяется от 0 до 1, 1 - полностью объясненная модель.\n",
    "\n",
    "**Adjusted R-Squared (cкорректированный коэффициент детерминации)** - это модифицированная версия коэффициента детерминации, которая корректирует количество независимых переменных в модели, обеспечивая более точную оценку объяснительной силы модели при сравнении моделей с разным количеством переменных. Если скорректированный R-квадрат уменьшается при добавлении дополнительных переменных, это говорит о том, что дополнительные переменные не вносят существенного вклада в модель и могут быть опущены.\n",
    "\n",
    "**F-статистика и Prob(F-статистика)** F-статистика используется для проверки общей значимости модели. Нулевая гипотеза заключается в том, что все коэффициенты (кроме константы) равны нулю, что означает, что модель не объясняет никакой дисперсии в целевой переменной. Значение p, связанное с F-статистикой, указывает на вероятность наблюдения F-статистики (или более экстремального значения), если бы нулевая гипотеза была верна. Небольшое значение p (обычно менее 0,05) указывает на то, что модель статистически значима, что означает, что по крайней мере одна из независимых переменных оказывает значительное влияние на целевую переменную.\n",
    "\n",
    "**AIC:** - информационный критерий Акаике, численное значение само по себе ничего не дает, используется для сравнения моделей - чем меньше, тем лучше. Формула $AIC = 2*k -2*log~likehood$\n",
    "\n",
    "**BIC** - Байесовский информационный критерий ($BIC = ln(n)*k -2*log~likehood$), аналогично чем меньше - тем лучше\n",
    "\n",
    "**Omnibus:** Тест оценивает совместную нормальность остатков. Более высокое значение предполагает отклонение от нормальности.\n",
    "\n",
    "**Prob(Omnibus)** - это p-value указывает на вероятность наблюдения тестовой статистики при нулевой гипотезе нормальности. Значение выше установленной границы (0.05 к примеру) предполагает, что мы не отвергаем нулевую гипотезу, подразумевая, что остатки могут быть распределены нормально.\n",
    "\n",
    "**Skew (ассиметрия)**:измеряет асимметрию распределения остатков. Значение асимметрии, близкое к нулю, указывает на симметричное распределение, тогда как положительные или отрицательные значения указывают на правый или левый перекос соответственно.\n",
    "\n",
    "**Kurtosis (эксцесс)**: Эксцесс измеряет «хвостость» распределения. Значение эксцесса 3 указывает на нормальное распределение, тогда как значения выше или ниже предполагают более тяжелые или более легкие хвосты соответственно.\n",
    "\n",
    "**Тест Жака-Бера (JB):** Тест Жака-Бера — это еще один тест на нормальность, который оценивает, соответствуют ли асимметрия и эксцесс выборки показателям нормального распределения.\n",
    "\n",
    "**Prob(JB):** это p-value оценивает нулевую гипотезу нормальности. Значение выше установленной границы (0.05 к примеру) указывает на то, что мы не отвергаем нулевую гипотезу.\n",
    "\n",
    "**Durbin-Watson:** Эта статистика проверяет наличие автокорреляции в остатках регрессионного анализа. Значения, близкие к 2, предполагают отсутствие автокорреляции, тогда как значения меньше 1 или больше 3 указывают на положительную или отрицательную автокорреляцию соответственно.\n",
    "\n",
    "**Cond. No.** оценивает мультиколлинеарность, где значения выше 30 указывают на потенциальные проблемы мультиколлинеарности среди независимых переменных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_partregress_grid(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(result, \"Experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(result, \"Experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = pd.read_csv('SalaryData.csv')\n",
    "salary = salary.rename(columns={'Experience Years': 'Experience'})\n",
    "y = salary['Salary']\n",
    "x = salary[['Experience']]\n",
    "y.iloc[15] += 10**5\n",
    "y.iloc[16] += 10**5\n",
    "y.iloc[18] += 10**5\n",
    "salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary.loc[15,'Salary'] += 10**5\n",
    "salary.loc[16,'Salary'] += 10**5\n",
    "salary.loc[17,'Salary'] += 10**5\n",
    "salary.loc[18,'Salary'] += 10**5\n",
    "\n",
    "salary.loc[35,'Salary'] /= 10\n",
    "salary.loc[36,'Salary'] /= 10\n",
    "salary.loc[37,'Salary'] /= 10\n",
    "salary.loc[38,'Salary'] /= 10\n",
    "\n",
    "\n",
    "sm.add_constant\n",
    "model = ols(\"Salary ~ Experience \", data=salary)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(result, \"Experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(result, \"Experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Множественная линейная регрессия\n",
    "\n",
    "Пусть $y$ - целевая переменная\n",
    "\n",
    "$$y=\\beta_0+\\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_k \\cdot x_k +\\epsilon$$ \n",
    "\n",
    "Подгоняем модель вида $$y=\\beta_0+\\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_k \\cdot x_k$$ \n",
    "\n",
    "\n",
    "Общим в этих примерах является то, что они линейны по отношению к неизвестным параметрам \n",
    "$\\beta_0,\\beta_1,...,\\beta_{p}$, поэтому модель называют линейной регрессионной моделью.\n",
    "\n",
    "Наиболее популярным методом получения оценок параметров $\\beta_0,\\beta_1,...,\\beta_{p}$. является так называемый метод наименьших квадратов. \n",
    "\n",
    "Метод заключается в минимизации суммы квадратов ошибок $\\epsilon^{T}\\epsilon=\\sum_{i}\\epsilon_{i}^2$ относительно вектора параметров $\\beta=[\\beta_0,\\beta_1₁,...,\\beta_p]$\n",
    "    \n",
    "Представим $\\epsilon^{T}\\epsilon$ в виде\n",
    "\n",
    "$$\\epsilon^{T}\\epsilon=(Y-X\\beta)^{T}(Y-X\\beta)=Y^{T}Y-2\\beta^{T}X^{T}Y+\\beta^{T}X^{T}X\\beta$$\n",
    "\n",
    "Продифференцируем $\\epsilon^{T}\\epsilon$ по $\\beta$. Пpиравняем полученную производную нулю получим\n",
    "\n",
    "$$-2X^{T}Y+2X^{T}X\\beta=0$$\n",
    "\n",
    "или\n",
    "\n",
    "$$X^{T}X\\beta=X^{T}Y$$ \n",
    "\n",
    "Последние уравнения называют нормальными уравнениями. Решение этого уравнения вектор $\\tilde\\beta$ называют оценкой наименьших квадратов. \n",
    "\n",
    "Простой проверкой нетрудно убедиться, что $\\tilde\\beta$ доставляет  минимум для $\\epsilon^{T}\\epsilon.\\epsilon^{T}\\epsilon$ обозначают как RSS.\n",
    "\n",
    "\n",
    "Вектор $\\widetilde{Y}=X\\tilde\\beta=\\tilde\\beta_0+\\tilde\\beta_1x_{t,1}+...\\tilde\\beta_{p}x_{t,p}$ называют регрессией, а вектор \n",
    "$ e = Y-\\widetilde Y$ называют остатками.\n",
    "\n",
    "$e=Y-\\widetilde{Y}=Y-X\\tilde\\beta$\n",
    "\n",
    "Отметим, что при известных из линейной алгебры условиях на матрицу $X^{T}X$ оценки $\\tilde\\beta$ и ошибки $e$ будут единственны.    \n",
    "\n",
    "### Свойства оценок наименьших квадратов\n",
    "\n",
    "1. Предположим, что вектор ошибок имеем нулевое математическое ожидание $E[\\epsilon]=0$. Тогда \n",
    "$E[\\tilde\\beta]=(X^{T}X)^{-1}X^{T}E[Y]=(X^{T}X)^{-1}X^{T}X\\beta=\\beta$\n",
    "\n",
    "Таким образом $\\tilde\\beta$ являются несмещеными оценками вектора $\\beta$.\n",
    "\n",
    "2. Предположим, что все $\\epsilon_{i}   i=1,...,n$ некоррелированы и имеют одинаковую дисперсию, т.е. $D[\\epsilon]=\\sigma^2I_{n}$ тогда\n",
    "\n",
    "$D[Y]=D[Y-X\\tilde\\beta]=D[\\epsilon]=\\sigma^2I_{n}$\n",
    "\n",
    "И\n",
    "\n",
    "$D[\\tilde\\beta]=D[(X^{T}X)^{-1}X^{T}Y]=(X^{T}X)^{-1}X^{T}D[Y]X(X^{T}X)^{-1}=\\sigma^2(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}=\\sigma^2(X^{T}X)^{-1}$\n",
    "\n",
    "Возникает вопрос почему именно $\\tilde\\beta$ наиболее популярна при оценке параметров в линейной регрессионной модели.\n",
    "Ответ на этот вопрос дает следующая теорема\n",
    "\n",
    "**Теорема.**\n",
    "\n",
    "Пусть $θ$ - оценка наименьших квадратов вектора $θ=X \\tilde\\beta$. Тогда в классе всех других линейных несмещенных оценок линейной комбинации $c^{T}θ$, оценка $c'θ$ является единственной оценкой с минимальной дисперсией.\n",
    "\n",
    "До сих пор мы не делали никаких предположений о распределении. Если предоположить независимость и нормальность распределения ошибок $\\epsilon_t\\sim N(0,\\sigma^2)$ то Rao в 1974 году доказал, что $c^{T}\\tilde\\beta$ имеет минимальную дисперсию не только в классе всех линейных несмещенных оценок, но и в классе всех несмещенных оценок (эффективность). В этом случае также оценка максимального правдоподобия совпадает с оценкой наименьших квадратов. В случае не нормальности рапределения ошибок условия при которых оценка наименьших будет является ассимптотически эффективной найдены были Cox в 1968 году.\n",
    "\n",
    "Дополнительно будем предполагать нормальность  и независимость ошибок $\\epsilon_{i} \\sim N(0,\\sigma^2)\\  или\\  \\epsilon∼N(0,I_{n}\\sigma^2)$\n",
    "\n",
    "**Теорема 2.** Если $Y\\sim N(X\\beta,I_{n}\\sigma^2)$ и матрица $X$ размера $(n⊗p)$   имеет ранг $p$, тогда\n",
    "\n",
    "A) $\\tilde\\beta \\sim N(\\beta,\\sigma^2(X^{T}X)^{-1})$\n",
    "\n",
    "B) $(\\tilde\\beta-\\beta)^{T}X(\\tilde \\beta-\\beta)/\\sigma^2\\sim\\chi^2_p$\n",
    "\n",
    "C) $\\tilde\\beta$ не зависит от  $s^2$ как случайные величины\n",
    "\n",
    "D) $RSS/\\sigma^2=(n-p)s^2/\\sigma^2∼\\chi_{n-p}²$    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = pd.read_csv('multipleSalaryData.csv')\n",
    "salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant\n",
    "model = ols(\"income ~ experience\", data=salary).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(\"income ~ age\", data=salary).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant\n",
    "model = ols(\"income ~ experience + age \", data=salary).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant\n",
    "model = ols(\"income ~ experience + age - 1\", data=salary).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(model,\"experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(model, \"experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(model, \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(model,\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv('multipleSalaryData.csv')\n",
    "X1, y1 = income.iloc[:, :-1].values, income.iloc[:, -1].values\n",
    "X1_scaled = scale(X1)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=0)\n",
    "X1_train_s, X1_test_s, y1_train, y1_test = train_test_split(X1_scaled, y1, random_state=0)\n",
    "sk_linear_regression = LinearRegression()\n",
    "sk_linear_regression.fit(X1_train, y1_train)\n",
    "\n",
    "sk_lr_pred_res = sk_linear_regression.predict(X1_test)\n",
    "sk_lr_r2 = r2_score(y1_test, sk_lr_pred_res)\n",
    "sk_lr_mape = mean_absolute_percentage_error(y1_test, sk_lr_pred_res)\n",
    "\n",
    "print(f'Scikit-learn Linear regression R2 score: {sk_lr_r2}')\n",
    "print(f'Scikit-learn Linear regression MAPE: {sk_lr_mape}', '\\n')\n",
    "\n",
    "print(f'weights: {sk_linear_regression.intercept_, *sk_linear_regression.coef_}')\n",
    "print(f'prediction: {sk_lr_pred_res}', '\\n')\n",
    "\n",
    "sk_linear_regression.fit(X1_train_s, y1_train)\n",
    "print(f'scaled weights: {sk_linear_regression.intercept_, *sk_linear_regression.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1, feature2 = X1[:, 0], X1[:, 1]\n",
    "X1_linspace = np.linspace(feature1.min(), feature1.max())\n",
    "X2_linspace = np.linspace(feature2.min(), feature2.max())\n",
    "X1_surface, X2_surface = np.meshgrid(X1_linspace, X2_linspace)\n",
    "X_surfaces = np.array([X1_surface.ravel(), X2_surface.ravel()]).T\n",
    "\n",
    "sk_linear_regression = LinearRegression()\n",
    "sk_linear_regression.fit(X1_train, y1_train)\n",
    "y_surface = sk_linear_regression.predict(X_surfaces).reshape(X1_surface.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(feature1, feature2, y1, color='red', marker='o')\n",
    "ax.plot_surface(X1_surface, X2_surface, y_surface, color='black', alpha=0.6)\n",
    "plt.title('Fitted linear regression surface')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Experience')\n",
    "ax.set_zlabel('Income')\n",
    "ax.view_init(20, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальная регрессия\n",
    "\n",
    "Важным случаем является подгонка полинома, то есть\n",
    "Подгоняем модель вида $$y=\\beta_0+\\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + \\dots + \\beta_k \\cdot x^k$$  \n",
    "С помощью МНК (метод наименьших квадратов) хотим найти оценки для всех коэффициентов. Нужно минимизировать\n",
    "$$S = \\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2$$\n",
    "$$S = \\sum\\limits_{i=1}^n (y_i - \\sum\\limits_{m=0}^k \\beta_m x_i^m)^2$$\n",
    "Приравняем все частные производные к нулю\n",
    "$$S^{'}_{\\beta_m} = 2\\sum\\limits_{i=1}^n x_i^m(\\sum\\limits_{m=0}^k \\beta_m x_i^m - y_i) = 0$$\n",
    "Раскроем внутреннюю сумму и перенесем y в правую часть\n",
    "$$\\sum\\limits_{i=1}^n x_i^m (\\beta_0 + \\beta_1 x_i + ... + \\beta_k x_i^k) = \\sum\\limits_{i=1}^n x_i^m y_i$$\n",
    "Разделим обе части каждого такого уравнения на n и запишем в виде системы\n",
    "$$\\begin{cases} \\beta_0 + \\beta_1\\overline{x} + ... \\beta_k\\overline{x^k} = \\overline{y}\\\\ \\beta_0\\overline{x} + \\beta_1\\overline{x^2} + ... \\beta_k\\overline{x^{k+1}} = \\overline{xy} \\\\ ... \\\\ \\beta_0\\overline{x^k} + \\beta_1\\overline{x^{k+1}} + ... \\beta_k\\overline{x^{2k}} = \\overline{x^ky} \\end{cases}$$\n",
    "\n",
    "Решением этой системы будет вектор из коэффициентов, на которых строится модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Ice_cream selling data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['Temperature (°C)'],data['Ice Cream Sales (units)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = data['Ice Cream Sales (units)']\n",
    "#x = data['Temperature (°C)']\n",
    "data = data.rename(columns = {'Ice Cream Sales (units)': 'ice','Temperature (°C)': 't'})\n",
    "sm.add_constant\n",
    "model = ols(\"ice ~ t\", data=data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(model, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Ice_cream selling data.csv')\n",
    "data = data.rename(columns = {'Ice Cream Sales (units)': 'ice','Temperature (°C)': 't'})\n",
    "data['t'] = data['t']**2\n",
    "sm.add_constant\n",
    "model = ols(\"ice ~ t\", data=data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_regress_exog(model, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_cream = pd.read_csv('Ice_cream selling data.csv')\n",
    "X2, y2 = ice_cream.iloc[:, :-1], ice_cream.iloc[:, -1]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=0)\n",
    "print(ice_cream.head())\n",
    "feature_name, target_name = ice_cream.columns\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=True)\n",
    "X2_poly = poly_features.fit_transform(X2)\n",
    "X2_poly_train, X2_poly_test = train_test_split(X2_poly, random_state=0)\n",
    "\n",
    "sk_linear_regression = LinearRegression()\n",
    "sk_linear_regression.fit(X2_train, y2_train)\n",
    "sk_lr_pred_res = sk_linear_regression.predict(X2_test)\n",
    "sk_lr_pred_all_data_res = sk_linear_regression.predict(X2)\n",
    "\n",
    "sk_polynomial_regression = LinearRegression()\n",
    "sk_polynomial_regression.fit(X2_poly_train, y2_train)\n",
    "sk_poly_lr_pred_res = sk_polynomial_regression.predict(X2_poly_test)\n",
    "sk_poly_lr_pred_all_data_res = sk_polynomial_regression.predict(X2_poly)\n",
    "\n",
    "linear_regression_r2 = r2_score(y2_test, sk_lr_pred_res)\n",
    "polynomial_regression_r2 = r2_score(y2_test, sk_poly_lr_pred_res)\n",
    "\n",
    "\n",
    "\n",
    "linear_regression_mse = mean_squared_error(y2_test, sk_lr_pred_res)\n",
    "polynomial_regression_mse = mean_squared_error(y2_test, sk_poly_lr_pred_res)\n",
    "\n",
    "linear_regression_mape = mean_absolute_percentage_error(y2_test, sk_lr_pred_res)\n",
    "polynomial_regression_mape = mean_absolute_percentage_error(y2_test, sk_poly_lr_pred_res)\n",
    "\n",
    "print(f'R2 score (Linear regression): {linear_regression_r2}')\n",
    "print(f'R2 score (Polynomial regression): {polynomial_regression_r2}', '\\n')\n",
    "\n",
    "print(f'MSE (Linear regression): {linear_regression_mse}')\n",
    "print(f'MSE (Polynomial regression): {polynomial_regression_mse}', '\\n')\n",
    "\n",
    "print(f'MAPE (Linear regression): {linear_regression_mape}')\n",
    "print(f'MAPE (Polynomial regression): {polynomial_regression_mape}')\n",
    "\n",
    "plt.scatter(X2, y2)\n",
    "plt.plot(X2, sk_lr_pred_all_data_res, color='darkorange', label='Linear Regression')\n",
    "plt.plot(X2, sk_poly_lr_pred_all_data_res, color='green', label='Polynomial Regression')\n",
    "plt.title('Polynomial vs Linear regression')\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel(target_name)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация в линейной регрессии\n",
    "\n",
    "$$J_{LASSO} = \\sum_i (y_i - \\hat{y})^2 + \\lambda ||w||$$\n",
    "\n",
    "$$J_{RIDGE} = \\sum_i (y_i - \\hat{y})^2 + \\lambda w^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.load_diabetes(as_frame=True, return_X_y=True)\n",
    "x = pd.DataFrame(x)\n",
    "sns.heatmap(x.corr(),annot=True,fmt=\".2f\", linewidth=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = x[['bmi']]\n",
    "# Импорт класса LinearRegression из модуля linear_model пакета scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Создание экземпляра класса LinearRegression\n",
    "simple_lr = LinearRegression()\n",
    "\n",
    "# Обучение модели\n",
    "simple_lr.fit(bmi, y)\n",
    "\n",
    "# Прогнозирование целевой переменной и сохранение результата в predicted_y\n",
    "predicted_y = simple_lr.predict(bmi)\n",
    "\n",
    "# Построение линии регрессии на точечной диаграмме\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(bmi, y)\n",
    "plt.plot(bmi, predicted_y, c = 'r')\n",
    "plt.title('Scatter plot and a Simple Linear Regression Model')\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"bmi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт функции cross_val_score из модуля model_selection scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Сохранение десяти метрик в переменной mse\n",
    "mse = cross_val_score(simple_lr,\n",
    "                      bmi,\n",
    "                      y,\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      cv=10)\n",
    "\n",
    "# Получение среднего значения для оценки качества модели\n",
    "mse.mean()\n",
    "# -3906.9189901068407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели на всех признаках\n",
    "multiple_lr = LinearRegression().fit(x, y)\n",
    "\n",
    "# Сохраняем массив с результатами оценки качества\n",
    "mse = cross_val_score(multiple_lr,\n",
    "                     x,\n",
    "                     y,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=10)\n",
    "\n",
    "# Получение среднего значения метрик для оценки качества модели\n",
    "mse.mean()\n",
    "# -3000.3902901608426\n",
    "\n",
    "# Сохранение массива, содержащего все десять коэффициентов\n",
    "multiple_lr_coeffs = multiple_lr.coef_\n",
    "multiple_lr_coeffs\n",
    "feature_names = x.columns\n",
    "\n",
    "# Используем matplotlib для построения графика\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(multiple_lr_coeffs)), multiple_lr_coeffs)\n",
    "plt.axhline(0, color = 'r', linestyle = 'solid')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation = 50)\n",
    "plt.title(\"Coefficients for Multiple Linear Regression\")\n",
    "plt.ylabel(\"coefficients\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт класса Ridge из модуля linear_model scikit-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Импорт класса GridSearchCV из модуля model_selection scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Создание словаря, содержащего потенциальные значения альфа\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]}\n",
    "\n",
    "# Передача в GridSearchCV Ridge-модели, потенциальных альфа-значений,\n",
    "# метрики качества\n",
    "ridge = GridSearchCV(Ridge(),\n",
    "                     alpha_values,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=10)\n",
    "\n",
    "# обучение модели \n",
    "print('Лучшее значение alpha:', ridge.fit(x,y).best_params_)\n",
    "\n",
    "# Вывод среднего значения neg_mean_squared_error\n",
    "print('Метрика качества:', ridge.fit(x,y).best_score_)\n",
    "\n",
    "# Лучшее значение alpha: {'alpha': 0.04}\n",
    "# Метрика качества: -2997.195810600043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание объекта, содержащего наилучшую модель\n",
    "best_ridge_model = Ridge(alpha=0.04)\n",
    "\n",
    "# Извлечение оценок коэффициентов для всех десяти признаков\n",
    "best_ridge_coeffs = best_ridge_model.fit(x, y).coef_\n",
    "\n",
    "# Построение графика c коэффициентами для всех десяти признаков\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(range(len(feature_names)), best_ridge_coeffs)\n",
    "plt.axhline(0, color = 'r', linestyle = 'solid')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation = 50)\n",
    "plt.title(\"Coefficient estimates from Ridge Regression\")\n",
    "plt.ylabel(\"coefficients\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт класса Lasso из модуля linear_model scikit-learn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Создание словаря, содержащего потенциальные значения альфа\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,0.07, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]}\n",
    "\n",
    "# Передача в GridSearchCV модели, потенциальных альфа-значений,\n",
    "# метрики качества\n",
    "lasso = GridSearchCV(Lasso(),\n",
    "                     alpha_values,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=10)\n",
    "\n",
    "# Обучение модели \n",
    "print('Лучшее значение alpha:', lasso.fit(x, y).best_params_)\n",
    "\n",
    "# Вывод среднего значения neg_mean_squared_error \n",
    "print('Метрика качества:', lasso.fit(x, y).best_score_)\n",
    "\n",
    "# Лучшее значение alpha: {'alpha': 0.06}\n",
    "# Метрика качества:: -2987.4275179741567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание объекта, содержащего наилучшую Lasso-модель \n",
    "best_lasso_model = Lasso(alpha=0.06)\n",
    "\n",
    "# сохранение значений коэффициентов для всех десяти признаков\n",
    "best_lasso_coeffs = best_lasso_model.fit(x, y).coef_\n",
    "\n",
    "# Построение графика значений коэффициентов для всех десяти объектов\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(range(len(feature_names)), best_lasso_coeffs)\n",
    "plt.axhline(0, color = 'r', linestyle = 'solid')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation = 50)\n",
    "plt.title(\"Coefficient estimates from Lasso Regression\")\n",
    "plt.ylabel(\"coefficients\")\n",
    "plt.xlabel(\"features\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
